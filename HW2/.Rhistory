df<-filter(df, ntoken(df$text)>3)
df$read_FRE<-readability(df$text, "Flesch")
df$read_DC<-readability(df$text, "Dale.Chall")
cor(df$read_FRE, df$read_DC)
year_FRE<-data.frame(matrix(ncol = 23, nrow = 100))
for(i in 1:100){
#sample 2000
bootstrapped_year<-sample_n(df, 3000, replace=TRUE)
bootstrapped_year$read_FRE<-readability(bootstrapped_year$text, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped_year$read_FRE, by=list(bootstrapped_year$year), FUN=mean)[,2]
length(aggregate(bootstrapped_year$read_FRE, by=list(bootstrapped_year$year), FUN=mean)[,2])
}
colnames(year_FRE)<-names(table(df$year))
#define the standard error function
std <- function(x) sd(x)/sqrt(length(x))
##calculate standard errors and point estimates
year_ses<-apply(year_FRE, 2, std)
year_means<-apply(year_FRE, 2, mean)
#graphs --- year
coefs<-year_means
ses<-year_ses
y.axis <- c(1:23)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(year_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,23.5), main = "")
#rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
#rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
#rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
observed<-aggregate(df$read_FRE, by=list(df$year), FUN=mean)[,2]
points(observed, y.axis+2*adjust,pch=13,cex=.8, bg="white")
length(aggregate(bootstrapped_year$read_FRE, by=list(bootstrapped_year$year), FUN=mean)[,2])
xx<-tokenize(df$text, what = "sentence")
xx[1]
xx[2]
text <- lapply(files, readLines)
files <- list.files( full.names=TRUE)
text <- lapply(files, readLines)
setwd("C:/Users/kevin/Documents/GitHub/Text_as_Data/cons")
files <- list.files( full.names=TRUE)
text <- lapply(files, readLines)
files<-unlist(files)
files<-gsub("./", "", files )
files<-gsub(".txt", "", files )
df<-data.frame(year = rep((files), sapply(text, length)), text = (unlist(text)), stringsAsFactors = FALSE)
df<-filter(df, grepl("^\\Ã¯", df$text)==FALSE & grepl("^\\d", df$text) ==FALSE)
df<-filter(df, ntoken(df$text)>3)
df$read_FRE<-readability(df$text, "Flesch")
df$read_DC<-readability(df$text, "Dale.Chall")
xx[2]
tokens<-vector()
tokens<-c(tokens, tokenize(df$text, what = "sentence"))
for(i in 1:5){
tokens<-c(tokens, tokenize(df$text[i], what = "sentence"))
}
tokens<-vector()
for(i in 1:5){
tokens<-c(tokens, tokenize(df$text[i], what = "sentence"))
}
tokens
tokens<-vector()
tokens<-vector()
for(i in 1:5){
tokens<-c(tokens, unlist(tokenize(df$text[i], what = "sentence")))
}
tokens
tokens<-vector()
labels<-vector()
for(i in 1:5){
tokens<-c(tokens, unlist(tokenize(df$text[i], what = "sentence")))
labels<-c(labels, length(unlist(tokenize(df$text[i], what = "sentence"))))
}
labels
tokens<-vector()
labels<-vector()
for(i in 1:5){
tokens<-c(tokens, unlist(tokenize(df$text[i], what = "sentence")))
labels<-c(labels, rep(df$years[i], length(unlist(tokenize(df$text[i], what = "sentence")))))
}
labels
length(unlist(tokenize(df$text[i], what = "sentence")))
rep(df$years[i], length(unlist(tokenize(df$text[i], what = "sentence"))))
tokens<-vector()
labels<-vector()
for(i in 1:5){
tokens<-c(tokens, unlist(tokenize(df$text[i], what = "sentence")))
labels<-c(labels, rep(df$year[i], length(unlist(tokenize(df$text[i], what = "sentence")))))
}
labels
tokens<-vector()
labels<-vector()
for(i in 1:length(df$text){
tokens<-c(tokens, unlist(tokenize(df$text[i], what = "sentence")))
labels<-c(labels, rep(df$year[i], length(unlist(tokenize(df$text[i], what = "sentence")))))
}
tokens<-vector()
labels<-vector()
for(i in 1:length(df$text)){
tokens<-c(tokens, unlist(tokenize(df$text[i], what = "sentence")))
labels<-c(labels, rep(df$year[i], length(unlist(tokenize(df$text[i], what = "sentence")))))
}
df_tokens<-data.frame(tokens, labels)
df_tokens<-data.frame(tokens, stringsAsFactors = FALSE)
df_tokens$year<-labels
df_tokens$year<-as.factor(labels)
df_tokens$read_FRE<-readability(df_tokens$tokens, "Flesch")
df_tokens<-filter(df_tokens, ntoken(df_tokens$tokens)>3)
df_tokens$read_FRE<-readability(df_tokens$tokens, "Flesch")
df_tokens$read_DC<-readability(df_tokens$tokens, "Dale.Chall")
cor(df_tokens$read_FRE, df_tokens$read_DC)
year_FRE<-data.frame(matrix(ncol = 23, nrow = 100))
for(i in 1:100){
#sample 2000
bootstrapped_year<-sample_n(df_tokens, 3000, replace=TRUE)
bootstrapped_year$read_FRE<-readability(bootstrapped_year$tokens, "Flesch")
#store results
year_FRE[i,]<-aggregate(bootstrapped_year$read_FRE, by=list(bootstrapped_year$year), FUN=mean)[,2]
length(aggregate(bootstrapped_year$read_FRE, by=list(bootstrapped_year$year), FUN=mean)[,2])
}
colnames(year_FRE)<-names(table(df_tokens$year))
std <- function(x) sd(x)/sqrt(length(x))
year_ses<-apply(year_FRE, 2, std)
year_means<-apply(year_FRE, 2, mean)
coefs<-year_means
ses<-year_ses
y.axis <- c(1:23)
min <- min(coefs - 2*ses)
max <- max(coefs + 2*ses)
var.names <- colnames(year_FRE)
adjust <- 0
par(mar=c(2,8,2,2))
#setwd("C:/Users/kevin/Dropbox/tokens_As_Data_Spring_2016_SpirlingMunger/homeworks")
pdf_tokens("Q6.pdf_tokens", 7, 5)
plot(coefs, y.axis, type = "p", axes = F, xlab = "", ylab = "", pch = 19, cex = .8,
xlim=c(min,max),ylim = c(.5,23.5), main = "")
#rect(min,.5,max,1.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,1.5,max,2.5, col = c("grey95"), border="grey90", lty = 2)
#rect(min,2.5,max,3.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,3.5,max,4.5, col = c("grey95"), border="grey90", lty = 2)
#rect(min,4.5,max,5.5, col = c("grey97"), border="grey90", lty = 2)
#rect(min,5.5,max,6.5, col = c("grey97"), border="grey90", lty = 2)
axis(1, at = seq(min,max,(max-min)/10),
labels = c(round(min+0*((max-min)/10),3),
round(min+1*((max-min)/10),3),
round(min+2*((max-min)/10),3),
round(min+3*((max-min)/10),3),
round(min+4*((max-min)/10),3),
round(min+5*((max-min)/10),3),
round(min+6*((max-min)/10),3),
round(min+7*((max-min)/10),3),
round(min+8*((max-min)/10),3),
round(min+9*((max-min)/10),3),
round(max,3)),tick = T,cex.axis = .75, mgp = c(2,.7,0))
axis(2, at = y.axis, label = var.names, las = 1, tick = FALSE, cex.axis =.8)
abline(h = y.axis, lty = 2, lwd = .5, col = "white")
segments(coefs-qnorm(.975)*ses, y.axis+2*adjust, coefs+qnorm(.975)*ses, y.axis+2*adjust, lwd =  1)
segments(coefs-qnorm(.95)*ses, y.axis+2*adjust-.035, coefs-qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
segments(coefs+qnorm(.95)*ses, y.axis+2*adjust-.035, coefs+qnorm(.95)*ses, y.axis+2*adjust+.035, lwd = .9)
points(coefs, y.axis+2*adjust,pch=21,cex=.8, bg="white")
observed<-aggregate(df_tokens$read_FRE, by=list(df_tokens$year), FUN=mean)[,2]
points(observed, y.axis+2*adjust,pch=13,cex=.8, bg="white")
?textmodel
textmodel(snippets_dfm, model="kNN")
install.packages("class")
library(class)
?knn
knn(train=snippets_dfm, test = mystery_dfm, cl = authors, k=1 )
knn_1<-knn(train=snippets_dfm, test = mystery_dfm, cl = authors, k=1 )
knn_1<-knn(train=snippets_dfm, test = mystery_dfm, cl = authors, k=3 )
knn_3<-knn(train=snippets_dfm, test = mystery_dfm, cl = authors, k=3 )
knn_3
knn_10<-knn(train=snippets_dfm, test = mystery_dfm, cl = authors, k=10 )
knn_10
rm(list=ls())
setwd("c:/Users/as9934/Dropbox/Text_As_Data_Spring_2016_SpirlingMunger/homeworks/random_forests")
setwd("c:/Users/kevin/Dropbox/Text_As_Data_Spring_2016_SpirlingMunger/homeworks/random_forests")
load("treaties.rdata")
require(randomForest)
mod.rf<-randomForest(treaty_DTM,treaty_harshness,ntree=500,importance=T)
varImpPlot(mod.rf, type=1)
?randomForest
?varImpPlot
rm(list=ls())
setwd("c:/Users/kevin/Dropbox/Text_As_Data_Spring_2016_SpirlingMunger/homeworks/random_forests")
load("treaties.rdata")
setwd("c:/Users/kevin/Dropbox/Text_As_Data_Spring_2016_SpirlingMunger/homeworks/random_forests")
#note that this data is tf-idf weighted
load("treaties.rdata")
#run random forest with
# x= the document term matrix (note odd syntax where x come first)
# y= estimated treaty harshness
require(randomForest)
mod.rf<-randomForest(treaty_DTM,treaty_harshness,ntree=500,importance=T)
?varImpPlot
varImpPlot(mod.rf, type=1)
?randomForest
set.seed(240)
mod.rf<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf, type=1)
set.seed(2400)
mod.rf<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf, type=1)
mod.rf_1<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_1, type=1)
varImpPlot(mod.rf_2, type=1)
mod.rf_2<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_1, type=1)
varImpPlot(mod.rf_2, type=1)
set.seed(2400)
mod.rf_2<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_2, type=1)
mod.rf_2<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_2, type=1)
mod.rf_2<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_2, type=1)
mod.rf_2<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_2, type=1)
set.seed(2400)
mod.rf_2<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_2, type=1)
set.seed(240)
mod.rf_2<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_2, type=1)
set.seed(240)
mod.rf_2<-randomForest(treaty_DTM,treaty_harshness,ntree=10,importance=T)
varImpPlot(mod.rf_2, type=1)
which.min(treaty_harshness)
min_dfm<-treaty_DTM[which.min(treaty_harshness)]
min_dfm<-treaty_DTM$Docs[which.min(treaty_harshness)]
min_dfm<-treaty_DTM$Docs[which.min(treaty_harshness),]
treaty_DTM$Docs
min_dfm<-treaty_DTM[which.min(treaty_harshness)]
ws_base<-textmodel(treaty_DTM, y=treaty_harshness,
model="wordscores")
library(quanteda)
ws_base<-textmodel(treaty_DTM, y=treaty_harshness,
model="wordscores")
dfm(treaty_DFM)
dfm(treaty_DTM)
?dfm
corpus(treaty_DTM)
library(quantedaData)
library(quanteda)
library(foreign)
setwd("C:/Users/k/Documents/GitHub/Text_as_Data/HW2/")
###Code for Question 2:
reviews<-read.csv("p4k_reviews.csv")
reviews$text<-as.character(reviews$text)
##divide up
med<-median(reviews$score)
ninety<-quantile(reviews$score, .9)
ten<-quantile(reviews$score, .1)
reviews$positive<-reviews$score>med
reviews$anchor_positive<-reviews$score>ninety
reviews$anchor_negative<-reviews$score<ten
table(reviews$anchor_negative)
setwd("C:/Users/kevin/Documents/GitHub/Text_as_Data/HW2/")
reviews<-read.csv("p4k_reviews.csv")
reviews$text<-as.character(reviews$text)
##divide up
med<-median(reviews$score)
ninety<-quantile(reviews$score, .9)
ten<-quantile(reviews$score, .1)
reviews$positive<-reviews$score>med
reviews$anchor_positive<-reviews$score>ninety
reviews$anchor_negative<-reviews$score<ten
table(reviews$anchor_negative)
#read in terms
pos<-read.table("positive-words.txt", as.is=F)
pos<-as.character(pos$V1)
neg<-read.table("negative-words.txt", as.is=F)
neg<-as.character(neg$V1)
##create dict
mydict <- list(pos=pos, neg=neg)
##pre-process
head(reviews$text)
mydfm<-dfm(reviews$text, removePunct=TRUE, stem=F, dictionary=mydict)
##calculate sentiment
reviews$sent<-as.numeric(mydfm[,'pos'])-as.numeric(mydfm[,'neg'])
table(reviews$sent)
reviews$sent_pos<-reviews$sent>-1
##find median sentiment
med_sent<-median(reviews$sent)
##find number of positive sentiment reviews
pos_sent<-length(which(reviews$sent_pos==TRUE))
##confusion matrix
table(reviews$positive, reviews$sent_pos)
reviews<-read.csv("p4k_reviews.csv")
reviews$text<-as.character(reviews$text)
med<-median(reviews$score)
ninety<-quantile(reviews$score, .9)
ten<-quantile(reviews$score, .1)
reviews$positive<-reviews$score>med
reviews$anchor_positive<-reviews$score>ninety
reviews$anchor_negative<-reviews$score<ten
table(reviews$anchor_negative)
#read in terms
pos<-read.table("positive-words.txt", as.is=F)
library(foreign)
pos<-read.table("positive-words.txt", as.is=F)
pos<-read.table("positive-words.txt", as.is=F)
pos<-as.character(pos$V1)
neg<-read.table("negative-words.txt", as.is=F)
neg<-as.character(neg$V1)
mydict <- list(pos=pos, neg=neg)
##pre-process
head(reviews$text)
mydfm<-dfm(reviews$text, removePunct=TRUE, stem=F, dictionary=mydict)
##calculate sentiment
reviews$sent<-as.numeric(mydfm[,'pos'])-as.numeric(mydfm[,'neg'])
table(reviews$sent)
reviews$sent_pos<-reviews$sent>-1
##find median sentiment
med_sent<-median(reviews$sent)
##find number of positive sentiment reviews
pos_sent<-length(which(reviews$sent_pos==TRUE))
##confusion matrix
table(reviews$positive, reviews$sent_pos)
acc<-(4413 + 174) /10000
mydict <- list(pos=pos, neg=neg)
precision<-4413/(4413+5219)
acc
precision
recall
recall<-4413/(4413+194)
recall
pos_sent
reviews$rank_scores<-rank(reviews$score, ties.method="average")
reviews$rank_sent<-rank(reviews$sent, ties.method="average")
rank_gap<-sum(abs(reviews$rank_scores - reviews$rank_sent))
rank_gap
reviews$label_NB<-reviews$positive
reviews$label_NB[2001:10000]<-NA
NBdfm<-dfm(reviews$text, stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
p4kNBmodel <- textmodel(NBdfm, reviews$label_NB,  model="NB", smooth=0, priors="uniform")
table(reviews$label_NB)
require(quantedaData)
data(amicusCorpus)
summary(amicusCorpus)
docvars(amicusCorpus, "trainclass")
factor(reviews$label_NB)
reviews$label_NB<-factor(reviews$label_NB)
NBdfm<-dfm(reviews$text, stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
p4kNBmodel <- textmodel(NBdfm, reviews$label_NB,  model="NB", smooth=0, priors="uniform")
print(p4kNBmodel, 10)
p4kpredict <- predict(p4kNBmodel)
reviews$train_NB<-reviews$positive
reviews$train_NB[1:2000]<-NA
reviews$train_NB<-factor(reviews$train_NB)
p4kNBmodel <- textmodel(NBdfm, reviews$label_NB,  model="NB", smooth=0, priors="uniform")
p4kpredict <- predict(p4kNBmodel)
reviews$train_NB
reviews$label_NB<-factor(reviews$label_NB)
reviews$label_NB<-reviews$positive
reviews$label_NB[2001:10000]<-NA
reviews$label_NB<-factor(reviews$label_NB)
reviews$label_NB
summary(amicusCorpus)
reviews$train_NB[reviews$train_NB=="TRUE"]<-"AP"
reviews$train_NB
reviews$label_NB
summary(reviews)
reviews$train_NB<-reviews$positive
reviews$train_NB[1:2000]<-NA
reviews$train_NB<-factor(reviews$train_NB)
summary(reviews$train_NB)
NBdfm<-dfm(reviews$text, stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
p4kNBmodel <- textmodel(NBdfm, reviews$label_NB,  model="NB", smooth=0, priors="uniform")
p4kpredict <- predict(p4kNBmodel)
docvars(amicusCorpus, "trainclass")
reviews$label_NB
reviews$label_NB<-as.numeric(reviews$positive)
reviews$label_NB[2001:10000]<-NA
reviews$label_NB<-factor(reviews$label_NB)
p4kNBmodel <- textmodel(NBdfm, reviews$label_NB,  model="NB", smooth=0, priors="uniform")
p4kpredict <- predict(p4kNBmodel)
amNBmodel <- textmodel(amDfm, docvars(amicusCorpus, "trainclass"), model="NB", smooth=1)
amDfm <- dfm(amicusCorpus)
amNBmodel <- textmodel(amDfm, docvars(amicusCorpus, "trainclass"), model="NB", smooth=1)
print(amNBmodel, 10)
(amNBpredict <- predict(amNBmodel))
p4kNBmodel
p4kpredict <- predict(p4kNBmodel, newx = reviews$label_NB)
p4kNBmodel
label_NB<-factor(reviews$positive)
label_NB[2001:10000]<-NA
label_NB
label_NB<-label_NB[2001:10000]
label_NB
label_NB<-factor(reviews$positive)
label_NB<-label_NB[2001:10000]
label_NB
train_NB<-reviews$positive
train_NB<-train_NB[1:2000]
p4kNBmodel <- textmodel(NBdfm, train_NB,  model="NB", smooth=0, priors="uniform")
p4kNBmodel <- textmodel(NBdfm[1:2000], train_NB,  model="NB", smooth=0, priors="uniform")
NBdfm[,1:2000]
NBdfm[1:2000,]
p4kNBmodel <- textmodel(NBdfm[1:2000,], train_NB,  model="NB", smooth=0, priors="uniform")
p4kNBmodel <- textmodel(NBdfm, train_NB,  model="NB", smooth=0, priors="uniform")
p4kNBmodel <- textmodel(NBdfm, reviews$train_NB,  model="NB", smooth=0, priors="uniform")
p4kNBmodel <- textmodel(NBdfm, reviews$label_NB,  model="NB", smooth=0, priors="uniform")
p4kpredict <- predict(p4kNBmodel)
p4kpredict <- predict(p4kNBmodel, newx = NBdfm[2001:10000,])
reviews$label_NB
?predict
summary(amicusCorpus)
?docvars
docvars(mydfm, "Label") <- factor(reviews$positive)
docvars(amicusCorpus, "trainclass")
p4kNBmodel <- textmodel(NBdfm, reviews$label_NB,  model="NB", smooth=1, priors="uniform")
p4kpredict <- predict(p4kNBmodel, newx = NBdfm)
p4kpredict <- predict(p4kNBmodel)
reviews$label_NB
NBdfm_train<-dfm(reviews$text[1:2000], stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
p4kNBmodel <- textmodel(NBdfm_train, reviews$label_NB[1:2000],  model="NB", smooth=1, priors="uniform")
NBdfm_test<-dfm(reviews$text[2001:10000], stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
p4kpredict <- predict(p4kNBmodel, newx=NBdfm_test)
p4kNBmodel <- textmodel(NBdfm_train, reviews$label_NB[1:2000],  model="NB", smooth=0, priors="uniform")
p4kpredict <- predict(p4kNBmodel, newx=NBdfm_test)
reviews$label_NB<-factor(as.numeric)reviews$positive))
reviews$label_NB<-factor(as.numeric(reviews$positive))
p4kNBmodel <- textmodel(NBdfm_train, reviews$label_NB[1:2000],  model="NB", smooth=0, priors="uniform")
p4kpredict <- predict(p4kNBmodel, newx=NBdfm_test)
reviews_pos<-filter(reviews, anchor_positive ==TRUE)
reviews_neg<-filter(reviews, anchor_negative ==TRUE)
NBdfm_test<-dfm(reviews_pos$text, stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
NBdfm_test[1,]
NBdfm_test[1,1]
?dfm
?tfIdf
?tfidf
NBdfm_pos<-dfm(reviews_pos$text, stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
NBdfm_pos<-tf(NBdfm_pos)
NBdfm_pos[1,1]
NBdfm_pos<-tf(NBdfm_pos)
?tf
NBdfm_pos[1,2]
tf(NBdfm_pos)
NBdfm_pos<-tf(NBdfm_pos, scheme= "prop")
NBdfm_pos[1,2]
NBdfm_pos[1,3]
NBdfm_pos[1,4]
tNBdfm_pos <- t(NBdfm_pos)
Pwr <- tNBdfm_pos / rowSums(tNBdfm_pos)
Sw <- Pwr %*% setscores
reviews_neg<-filter(reviews, anchor_negative ==TRUE)
reviews_anchor<-rbind(reviews_pos, reviews_neg)
scores<-seq(1, length(reviews_pos$text))
scores<-seq(rep(1, length(reviews_pos$text)))
rep(1, length(reviews_pos$text)
)
scores<-rep(1, length(reviews_pos$text))
scores<-c(rep(1, length(reviews_pos$text)), rep(1, length(reviews_neg)$text))
scores<-c(rep(1, length(reviews_pos$text)), rep(1, length(reviews_pos$text)))
Sw <- Pwr %*% scores
scores<-c(rep(1, length(reviews_pos$text)), rep(-1, length(reviews_pos$text)))
Sw <- Pwr %*% scores
NBdfm_ref<-dfm(reviews_pos$text, stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
##convert to term frequency
NBdfm_ref<-tf(NBdfm_ref, scheme= "prop")
##compute posterior word probability
tNBdfm_ref <- t(NBdfm_ref)
Pwr <- tNBdfm_ref / rowSums(tNBdfm_ref)
Sw <- Pwr %*% scores
NBdfm_ref<-dfm(reviews_anchor$text, stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
NBdfm_ref<-tf(NBdfm_ref, scheme= "prop")
##compute posterior word probability
tNBdfm_ref <- t(NBdfm_ref)
Pwr <- tNBdfm_ref / rowSums(tNBdfm_ref)
##compute likelihoods
Sw <- Pwr %*% scores
Pwr
scores
Pwr
Sw <- Pwr %*% scores
library(devtools)
install.github("quanteda")
install_github("quanteda")
install_github("kbenoit/quanteda")
library(quanteda)
reviews_pos<-filter(reviews, anchor_positive ==TRUE)
reviews_neg<-filter(reviews, anchor_negative ==TRUE)
reviews_anchor<-rbind(reviews_pos, reviews_neg)
scores<-c(rep(1, length(reviews_pos$text)), rep(-1, length(reviews_pos$text)))
scores
NBdfm_ref<-dfm(reviews_anchor$text, stem=T,  removePunct=T, ignoredFeatures=stopwords("english"))
